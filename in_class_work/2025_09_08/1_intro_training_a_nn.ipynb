{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class work 1: Introduction to training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we've covered some data science basics, let's get a very baseline neural network training pipeline going. \n",
    "\n",
    "**Important:** Make sure this notebook is pointing toward your custom `conda` kernel (top right drop-down menu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_0'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">0. Installing pytorch</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation recipes can change over time for popular ML repositories, so instead of giving you a formula to memorize here, I'm going to recommend you go straight to the source. Find the Pytorch documentation and look at the installation instructions. Make sure you can run the cell below before proceeding.\n",
    "\n",
    "**NOTE:** do not use `pip3 install` directly in a notebook cell -- refer to the previous notebook for guidance on how to install packages directly from a notebook cell, or use a Terminal window instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test that torch will work \n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here's the full cell of imports \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "### set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_1'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">1. Generating a dataset</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll generate one of the simplest toy datasets there is: two Gaussian distributions. Let's say that they describe our signal and our background events, but that there are only 2 parameters we can measure ($\\theta_1$ and $\\theta_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two 2D Gaussian distributions with the same number of events\n",
    "n_events = 2000\n",
    "\n",
    "### Background is sampled from N(mu0, Sigma0)\n",
    "\n",
    "mu0 = np.array([-1.0, -1.0])\n",
    "cov0 = np.array([[1.0, 0.2],\n",
    "                 [0.2, 1.2]])\n",
    "x0 = np.random.multivariate_normal(mu0, cov0, size=n_events)\n",
    "y0 = np.zeros((n_events, 1), dtype=np.float32)\n",
    "\n",
    "### Signal is sampled from N(mu1, Sigma1)\n",
    "mu1 = np.array([1.0, 1.0])\n",
    "cov1 = np.array([[1.3, -0.3],\n",
    "                 [-0.3, 0.8]])\n",
    "x1 = np.random.multivariate_normal(mu1, cov1, size=n_events)\n",
    "y1 = np.ones((n_events, 1), dtype=np.float32)\n",
    "\n",
    "X = np.vstack([x0, x1]).astype(np.float32)\n",
    "y = np.vstack([y0, y1]).astype(np.float32)\n",
    "\n",
    "### Put these together and shuffle\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx] # inputs\n",
    "y = y[idx] # labels\n",
    "\n",
    "print(\"Data shapes:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it into a Pandas dataframe for legibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack([X,y]), columns = [\"theta_1\", \"theta_2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df[df.label != True].theta_1, df[df.label != True].theta_2, s=6, color=\"grey\", alpha=0.5, label=\"Background\")\n",
    "plt.scatter(df[df.label == True].theta_1, df[df.label == True].theta_2, s=6, color=\"crimson\", alpha=0.5, label=\"Signal\")\n",
    "plt.legend()\n",
    "plt.title(\"Visualizing our datasets\")\n",
    "plt.xlabel(r\"$\\theta_1$\"); plt.ylabel(r\"$\\theta_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_2'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">2. Defining a neural network model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # output is 1-dimensional for binary classification\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.MLP(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_3'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">3. Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key steps include splitting into train/validation/test sets, standardizing data, and building dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"label\"])  # only look at the features, not the labels\n",
    "y = df[\"label\"]                 # labels\n",
    "\n",
    "### Take the first 70% for training\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(\n",
    "    X, y, test_size=0.3)\n",
    "\n",
    "### Then evenly split the remaining 30% into validation & test set \n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_valtest, y_valtest, test_size=0.5\n",
    ")\n",
    "\n",
    "print(f\"Train set has {len(X_train)} events.\")\n",
    "print(f\"Validation set has {len(X_val)} events.\")\n",
    "print(f\"Test set has {len(X_test)} events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can standardize your data using a \"z-score\" manually by subtracting the mean and dividing by the standard deviation. \n",
    "\n",
    "Alternatively, you can use the StandardScaler plugin to do this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "### IMPORTANT -- you only want to define your scaling (\"fit_transform\") based on your training dataset.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "### Now re-use the same transformation on validation & test sets (\"transform\")\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train mean (before scaling):\", np.array(X_train.mean(axis=0)))\n",
    "print(\"Train mean (after scaling) -- each dimension should be close to 0:\", X_train_scaled.mean(axis=0))\n",
    "print(\"Train std (before scaling)\", np.array(X_train.std(axis=0)))\n",
    "print(\"Train std (after scaling) -- each dimension should be close to 1:\", X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### batch size is often an important hyperparameter. we'll use 32 as our default for now. \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_scaled), torch.from_numpy(y_train.values.astype(\"float32\")).view(-1, 1))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val_scaled),   torch.from_numpy(y_val.values.astype(\"float32\")).view(-1, 1))\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test_scaled),  torch.from_numpy(y_test.values.astype(\"float32\")).view(-1, 1))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test one of these loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch, y_batch in train_loader:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_4'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">4. Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with tracking our model training using `livelossplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for available GPUs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "### Initialize the model, optimizer, and loss function\n",
    "model = MLP(input_dim=2, hidden_dim=32).to(device)\n",
    "model = model.to(device) # move onto the GPU, if present\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "liveloss = PlotLosses(figsize=(9, 4)) \n",
    "logs = {}\n",
    "\n",
    "\n",
    "### Training loop\n",
    "for epoch in range(10):\n",
    "\n",
    "    ### train\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Move the batch onto the GPU, if present\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    train_loss_per_batch = total_train_loss / len(train_loader)\n",
    "    logs['loss'] = train_loss_per_batch\n",
    "\n",
    "    ### validate\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        \n",
    "        ### Move the batch onto the GPU, if present\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        total_val_loss += loss.item()\n",
    "    \n",
    "    val_loss_per_batch = total_val_loss / len(val_loader)\n",
    "    logs['val_loss'] = val_loss_per_batch\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    \n",
    "    liveloss.send()\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip this for now, but if you have time once you finish this notebook, try connecting the notebook to Wandb to have the option of tracking your runs: \n",
    "- https://docs.wandb.ai/guides/track/jupyter/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"phys-805-test\",\n",
    "#     config={\n",
    "#         \"batch_size\": 128,\n",
    "#         \"learning_rate\": 0.01,\n",
    "#         \"dataset\": \"gaussians\",\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_5'></a>\n",
    "<h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #1f77b4\">5. Evaluating model performance</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss alone might not be a very intuitive metric for understanding our model performance. Let's evaluate the model on our holdout test set and get some new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the threshold for your NN score (e.g. 50%)\n",
    "threshold = 0.5 \n",
    "\n",
    "### test set \n",
    "model.eval()\n",
    "test_loss, test_acc, count = 0.0, 0.0, 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        \n",
    "        ### Move the batch onto the GPU, if present\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "    \n",
    "        out = model(x_batch)\n",
    "        loss = loss_fn(out, y_batch)\n",
    "        test_loss += loss.item() * x_batch.size(0)\n",
    "        test_acc  += (torch.sigmoid(out) > threshold).float().eq(y_batch).float().mean().item() * x_batch.size(0)\n",
    "        count += x_batch.size(0)\n",
    "        \n",
    "print(f\"Average test loss: {test_loss/count:.4f} | Test accuracy: {test_acc/count:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further exercises:\n",
    "- What is a simple baseline you could define here to get context for how well your model is doing? Implement this and use it as a comparison.\n",
    "- Try modifying the hidden size, number of MLP layers, to see the effect on performance\n",
    "- Try increasing the dataset size\n",
    "- Try different learning rates\n",
    "- What's the effect of changing your batch size?\n",
    "- Try adding dropout\n",
    "- Try making the task harder by making the Gaussians overlap and/or shifting their positions. Where does your model start to break down?\n",
    "- Can you track an \"accuracy\" metric alongside the loss in your live tracking plots?\n",
    "- Can you visualize the decision boundary in parameter space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNjh80VUTQo5GkMNUc0DRQc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "phys_805",
   "language": "python",
   "name": "phys_805"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
