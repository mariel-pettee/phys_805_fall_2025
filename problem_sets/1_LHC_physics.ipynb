{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea2a340-72ab-49a1-935b-0ac155b8a2a8",
   "metadata": {},
   "source": [
    "# Problem Set 1: LHC data analysis with neural networks\n",
    "\n",
    "**Software requirements:**\n",
    "You will need a few special software packages, including `uproot` and `pylorentz`, for this problem set. (Both are installable via `pip`.)\n",
    "\n",
    "**Datasets:**\n",
    "I recommend creating a folder called something like `data/pset_1` wherever you are working on this problem set. The datasets can be downloaded at this link: https://uwmadison.box.com/s/bjlt381q7x4eufw70uq84nbx969onxn5 \n",
    "\n",
    "**Grading:**\n",
    "This problem set will be graded as a quiz within Canvas.\n",
    "\n",
    "**Deadline:** \n",
    "The Canvas quiz will close on Friday, September 19th at 11:59pm Central Time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a2dcb-3799-4b1a-a996-0ad5ad840c67",
   "metadata": {},
   "source": [
    "# Tau identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a57884-7218-4c7f-a37e-71c471daa3cd",
   "metadata": {},
   "source": [
    "This section uses highly realistic simulations that have actually been used to help develop new triggers at the LHC. These triggers need to make decisions ~40 million times per second! \n",
    "\n",
    "The signal sample consists of well-defined taus from simulated Higgs boson decays into tau pairs ($H\\rightarrow\\tau\\tau$). The background sample consists of a mix of unbiased background events and high-momentum QCD jets. Including both sources of backgrounds helps us to construct a model that works well even for high-momentum events.\n",
    "\n",
    "In these problems, you will train a **classification model** where the output is a probability score for whether the input object is a tau lepton or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b88e8d-9546-4ef1-a6c2-80ef27c4a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make sure to install `uproot` via pip into your environment before you start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a7846-de45-4fe0-9dbd-32b74dbbcf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uproot\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f5583-a71d-42f8-98a2-a60ca7463788",
   "metadata": {},
   "outputs": [],
   "source": [
    "### don't change this cell -- setting a seed will ensure that your answers are consistent \n",
    "seed = 1234 \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bf499-0982-4dc3-8d47-e9d4d45fe8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "### set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf528c-3a01-4875-9ba0-8a1273747996",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c177d-9ae6-4f13-bc51-3e291a88fe08",
   "metadata": {},
   "source": [
    "Note that `X` usually refers to the inputs and `y` refers to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af286ea-c56c-4627-9957-e6c4e0749f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_data(folder, n_vars=1, n_events=100_000):        \n",
    "    sig = uproot.open(folder+\"test_sig_v12_emseed.root\")\n",
    "    bkg = uproot.open(folder+\"test_bkg_v12_emseed.root\")\n",
    "    qcd = uproot.open(folder+\"test_qcd_v12_emseed.root\")\n",
    "\n",
    "    sig_input = sig['ntuplePupSingle']['tree']['m_inputs'].array()\n",
    "    bkg_input = bkg['ntuplePupSingle']['tree']['m_inputs'].array()\n",
    "    qcd_input = qcd['ntuplePupSingle']['tree']['m_inputs'].array()\n",
    "\n",
    "    truth_pt_sig = np.asarray(sig['ntuplePupSingle']['tree']['genpt1'].array())\n",
    "\n",
    "    reco_pt_sig  = sig['ntuplePupSingle']['tree']['pt'].array()\n",
    "    deltaR_sig   = sig['ntuplePupSingle']['tree']['gendr1'].array()\n",
    "    eta_sig      = sig['ntuplePupSingle']['tree']['geneta1'].array()\n",
    "    selection_sig = (reco_pt_sig > 12.) & (abs(deltaR_sig) < 0.4) & (abs(eta_sig) < 2.4)\n",
    "    selection_bkg = (bkg['ntuplePupSingle']['tree']['pt'].array() > 12) \n",
    "    selection_qcd = (qcd['ntuplePupSingle']['tree']['pt'].array() > 12)\n",
    "\n",
    "    # Inputs: pT, eta, phi, particle ID (one hot encoded)\n",
    "    X_sig = np.nan_to_num(np.asarray(sig_input[selection_sig][0:n_events]))\n",
    "    y_sig = np.full(X_sig.shape[0], 1.)\n",
    "\n",
    "    X_bkg    = np.nan_to_num(np.asarray(bkg_input)[selection_bkg][0:n_events])\n",
    "    y_bkg    = np.full(X_bkg.shape[0], 0.)\n",
    "    \n",
    "    X_qcd    = np.nan_to_num(np.asarray(qcd_input)[selection_qcd][0:n_events])\n",
    "    y_qcd    = np.full(X_qcd.shape[0], 0.)\n",
    "    \n",
    "    print(\"Signal Samples\",len(y_sig),\"label:\",y_sig[0])\n",
    "    print(\"Bkg Samples\",len(y_bkg),\"label:\",y_bkg[0])\n",
    "    print(\"QCD Samples\",len(y_qcd),\"label:\",y_qcd[0])\n",
    "    \n",
    "    X = np.concatenate([X_sig, X_bkg, X_qcd])\n",
    "    y = np.concatenate([y_sig, y_bkg, y_qcd])\n",
    "    \n",
    "    ### Data cleaning\n",
    "    X[abs(X) > 1e+9] = 0.    \n",
    "    if n_vars > 0:\n",
    "        X = X[:,0:n_vars]\n",
    "    \n",
    "    ### Convert to PyTorch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f569686-5b2f-41a8-bc5f-6af8b506608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_train_test_data(\"./data/pset_1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab39d2-886c-430c-a997-1241593df168",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 128, 50)\n",
    "plt.hist(X[:,0][y == 1], bins=bins, color=\"crimson\", label=\"signal\", alpha=0.5, density=True)\n",
    "plt.hist(X[:,0][y == 0], bins=bins, color=\"grey\", label=\"background\", alpha=0.5, density=True)\n",
    "plt.xlabel(r\"Leading Particle $p_T$\")\n",
    "plt.ylabel(r\"Normalized Counts\")\n",
    "plt.title(\"First input variable to the NN\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43120513-e7d2-4735-b4f6-5a9993dd4c69",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 1: Train/val/test splits</span>\n",
    "\n",
    "Now we want to make to split the dataset into 3 components so that we can validate the training:\n",
    "\n",
    "- **Training set:** Used to train the neural network's trainable parameters\n",
    "- **Validation set:** Used to check for signs of overtraining during the training process\n",
    "- **Test set:** Used to evaluate the performance of the trained model\n",
    "\n",
    "Using the code below as a starting point, split the data into: 60% training, 20% validation, and 20% test. How many events do you see in train, validation, and test? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f628a9-5e3b-4111-847a-6037f10e4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, train_frac=0.6):\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    # TIP: Check out the documentation here: https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "    train, val_and_test = torch.utils.data.random_split(dataset, [???, ???])\n",
    "    val, test           = torch.utils.data.random_split(val_and_test, [???, ???])\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f16e9a-f05f-45f7-8e46-506ce076c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train_val_test_split(X, y)\n",
    "print(f\"Training set has {len(train):,} events.\")\n",
    "print(f\"Validation set has {len(val):,} events.\")\n",
    "print(f\"Test set has {len(test):,} events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c40240-319c-4522-beac-188693e35498",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 2: Average score before training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e85eb7-bac0-4981-a794-34795e2c9ed4",
   "metadata": {},
   "source": [
    "Now we'll construct a neural network with a single layer (i.e. no hidden layers) that applies a sigmoid output. \n",
    "\n",
    "Create the network and then apply it directly to the test dataset. \n",
    "\n",
    "What is the average output value of this network, before we've even trained it? Report a number with precision 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa9b74-5020-47d4-aee7-e5bd8b7725b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_1layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = ???\n",
    "        self.sigmoid = ??? # apply a sigmoid activation here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = NN_1layer(input_dim = 1) # we're only using one input variable for now\n",
    "output = model(test[:][0])\n",
    "print(f\"{output.detach().cpu().numpy().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a3aa2-20e9-4b3c-9105-56048b345912",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 3: Average loss before training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76f348-6b77-4b33-83c1-ea4b9a210667",
   "metadata": {},
   "source": [
    "Calculate the binary cross-entropy loss on the test dataset (again, remember -- we haven't trained anything yet, so this is a randomly-initialized model). What is your loss? Report a number with precision 1e-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a654c-8d8f-4ed4-ae03-d2f9feb7dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c6d23-9570-429c-881b-6ee343df8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = ## Add binary cross entropy loss here\n",
    "loss = loss_fn(output.flatten(), y_true)\n",
    "print(f\"{loss.detach().cpu().numpy().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa22d6-0c29-4820-a84f-6cc77ff86821",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 4: Losses after 10 epochs</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f603302-ec29-4090-8948-17310dae057b",
   "metadata": {},
   "source": [
    "Now that we've developed an intuition for how well the model can perform with no training and with a single input variable (translation: not very well!), we'll train a full version of this model.\n",
    "\n",
    "What is the test loss after 10 epochs? Report your answer with precision 1e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c780bf-0d95-4094-8dd7-9f759b3c3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, train, val, n_epochs=5, learning_rate=0.01, batch_size=1000):\n",
    "    \n",
    "    optimizer = ??? ### add Adam optimizer with learning rate of 0.01 \n",
    "    liveloss = PlotLosses(figsize=(9, 4)) \n",
    "    logs     = {}\n",
    "\n",
    "    ### DataLoaders \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        current_loss = 0.0 # reset the loss at the start of each epoch\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            model.train()\n",
    "            x_batch, y_batch = batch\n",
    "\n",
    "            ??? ### zero out the optimizer\n",
    "            preds = model(x_batch).squeeze(1)\n",
    "            loss = ??? ### calculate the loss\n",
    "            ??? ### backpropagate the loss\n",
    "            ??? ### apply a step to the optimizer\n",
    "            current_loss += loss.item()\n",
    "        \n",
    "            if i == len(train_loader)-1:\n",
    "\n",
    "                ### validation loop\n",
    "                model.eval()\n",
    "                current_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for iv, val_batch in enumerate(val_loader):\n",
    "                        x_val_batch, y_val_batch = val_batch\n",
    "                        val_preds = ??? ### fill these in\n",
    "                        val_loss = ??? ### fill these in\n",
    "                        current_val_loss += val_loss.item()\n",
    "                    print('[%d, %4d] loss: %.4f  val loss: %.4f' % \n",
    "                          (epoch + 1, i + 1, current_loss/float(i+1) , current_val_loss/float(len(val_loader))))\n",
    "                logs['loss'] = current_loss/float(i+1)\n",
    "                logs['val_loss'] = current_val_loss/float(len(val_loader))\n",
    "\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "        \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b27b22-20fc-43ef-8ebc-8bb52f22a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_1layer(input_dim = 1) ### define a fresh instance of your model\n",
    "logs = train_model(model, loss_fn, train, val, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0a1ed-2a78-4f05-bc44-02028c5c4248",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9444e6-5ad7-43d3-8f56-a10ef8b5d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = ??? \n",
    "print(f\"Train loss: {logs['loss']:.2f}, Val loss: {logs['val_loss']:.2f}, Test loss: {test_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154860ea-ca71-467e-a334-5f3dbe958e9a",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 5: ROC Curve</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9a112-15d9-4ca7-9b04-2081de20d738",
   "metadata": {},
   "source": [
    "One way of understanding the performance of our model is to calculate the area under the curve (AUC) of the ROC curve. This curve is constructed by plotting the true positive rate, i.e. the signal efficiency, vs. the false positive rate, i.e. the background efficiency. A well-trained classifier will have a high signal efficiency and low background efficiency. Report the AUC of the ROC curve you calculate below with precision 1e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d48158-de21-4d2c-8b41-ed33a92e3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "preds = ??? ### compute the NN scores (i.e. final output values) on the test dataset\n",
    "labels = ??? ### labels on the test dataset\n",
    "\n",
    "bins = ??? ### 30 points spread between 0 and 1\n",
    "plt.hist(preds[labels==0].detach().numpy(), histtype='step',color='grey', density=True,label='background',bins=bins)\n",
    "plt.hist(preds[labels==1].detach().numpy(), histtype='step',color='crimson',density=True,label='signal',bins=bins)\n",
    "plt.xlabel('NN Score')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8cf4c-cc45-4955-87e9-62016b73a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ROC(labels, preds, n_points=501):\n",
    "    cut_vals = np.linspace(0, 1, n_points)\n",
    "    tot0 = float(len(labels[labels == 0]))  # number of background events\n",
    "    tot1 = float(len(labels[labels == 1]))  # number of signal events\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for c in cut_vals:\n",
    "        pass_bkg = ??? ### boolean indicating background and NN scores are larger than the cut value (\"c\")\n",
    "        pass_sig = ??? ### boolean indicating signal and NN scores are larger than the cut value (\"c\")\n",
    "        fpr.append(??? / tot0)\n",
    "        tpr.append(??? / tot1)\n",
    "    return np.array(fpr), np.array(tpr)\n",
    "\n",
    "fpr, tpr = compute_ROC(labels, preds)\n",
    "plt.plot(np.linspace(0,1,100), np.linspace(0,1,100), linestyle=\"dashed\", color=\"k\", label=\"Random guessing\")\n",
    "plt.plot(fpr, tpr,color=\"crimson\",label=f\"1-Layer NN (AUC = {metrics.auc(fpr, tpr):.2f})\")\n",
    "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n",
    "plt.xlabel(r\"False Positive Rate (FPR), i.e. $\\epsilon_{\\text{bkg}}$\")\n",
    "plt.ylabel(r\"True Positive Rate (TPR), i.e. $\\epsilon_{\\text{sig}}$\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC = {metrics.auc(fpr, tpr):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2209b-473f-4ecb-bf2d-cca418142d25",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 6: 3-layer NN on the full dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594d989-e78d-4538-811e-481a6186bfe9",
   "metadata": {},
   "source": [
    "We've now benchmarked the performance of this very simple one-layer neural network using just one input variable (!). Let's make a new model that's more expressive.\n",
    "\n",
    "Make a new model below that includes: \n",
    "- Training on all 80 of the input variables\n",
    "- 3 linear layers\n",
    "- Hidden dimension of 32 nodes\n",
    "- ReLU activations\n",
    "\n",
    "What is the test loss after 10 epochs? Report your answer with precision 1e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114947f3-6966-488d-9dd6-11df79bef8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_train_test_data(\"./data/pset_1/\", n_vars=-1) # -1 means we'll use all the available input variables\n",
    "train, val, test = ???\n",
    "print(f\"Training set has {len(train):,} events.\")\n",
    "print(f\"Validation set has {len(val):,} events.\")\n",
    "print(f\"Test set has {len(test):,} events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5720a-645f-4e15-88ce-74a836c9e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # you should see 80 as the second dimension, indicating 80 input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d55c40-dde6-4a10-ba4a-cf3a45685990",
   "metadata": {},
   "source": [
    "**Input variable definitions:** The 80 input variables to the NN correspond to 8 variables for the top 10 particles in each jet, ranked in descending order in momentum. \n",
    "\n",
    "The 8 variables for each of the 10 particles are: \n",
    "- pT \n",
    "- $\\theta$ (really $\\eta$) of the particle from the center of the jet\n",
    "- $\\phi$ of the particle from the center of the jet\n",
    "- 5 particle ID labels (one-hot encoded, i.e. 0 or 1): [pion, electron, muon, photon, neutral hadron])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd19d4e-3249-4206-8d7c-6dda78c850cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_3layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = ???\n",
    "        self.layer2 = ???\n",
    "        self.layer3 = ???\n",
    "        self.relu   = ???\n",
    "        self.output = ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        ???\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de6a9c-1959-4550-91ce-1880885ec1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_3layer(input_dim = 80, hidden_dim=32) ### define a fresh instance of your model\n",
    "logs = train_model(model, loss_fn, train, val, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13b28f-b17b-4cfe-a89b-3b2203bd1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = ???\n",
    "print(f\"Train loss: {logs['loss']:.2f}, Val loss: {logs['val_loss']:.2f}, Test loss: {test_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfa4db-88e5-40ef-9f67-f1863ba10552",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 7: 3-layer NN, 50 epochs</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af6f16-3be8-4182-a816-68a231ff2fb1",
   "metadata": {},
   "source": [
    "Train a new instance of the same model for 50 epochs instead of 10 epochs and report the final test loss value with precision 1e-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a30a6-85e5-4cbf-ab99-803dd5e39770",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 8: ROC AUC, round 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769b257-5887-44b5-aad6-3fc6f5e4e945",
   "metadata": {},
   "source": [
    "Now report the ROC AUC for the model you just trained for 50 epochs with precision 1e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bee20-53fb-417c-87d4-251b3227b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ???\n",
    "labels = ???\n",
    "\n",
    "bins = np.linspace(0,1,30)\n",
    "plt.hist(???, histtype='step',color='grey', density=True,label='background',bins=bins)\n",
    "plt.hist(???, histtype='step',color='crimson',density=True,label='signal',bins=bins)\n",
    "plt.xlabel('NN Score')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d0559-4535-4935-a36b-209e946c9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = ???\n",
    "plt.plot(np.linspace(0,1,100), np.linspace(0,1,100), linestyle=\"dashed\", color=\"k\", label=\"Random guessing\")\n",
    "plt.plot(fpr, tpr,color=\"crimson\",label=f\"3-Layer NN (AUC = {metrics.auc(fpr, tpr):.2f})\")\n",
    "plt.title(\"ROC (Receiver Operating Characteristic) Curve\")\n",
    "plt.xlabel(r\"False Positive Rate (FPR), i.e. $\\epsilon_{\\text{bkg}}$\")\n",
    "plt.ylabel(r\"True Positive Rate (TPR), i.e. $\\epsilon_{\\text{sig}}$\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "auc = ???\n",
    "print(f\"AUC = {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f8497-20f2-481c-b55c-1749e1e0dac5",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 9: Applying a cut to the classifier score</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974060a2-85cf-4ff7-af23-d72318d9e026",
   "metadata": {},
   "source": [
    "Now apply a cut on your NN classifier score at a threshold of 0.5 and compute the efficiency vs. leading particle pT (i.e. the first input). What is the NN doing? For example, examine the efficiency plot for pT = 30 GeV and pT = 100 GeV.\n",
    "\n",
    "Try this out, then consider which of the following statements reflect your observations. Select ALL that apply:\n",
    "\n",
    "- A) The signal efficiency drops quickly and has a low efficiency (~50%) for pT > 100 GeV\n",
    "- B) The signal efficiency increases rapidly as a function of leading pT until it goes above 80% and then stays about the same\n",
    "- C) The signal efficiency is always high (> 80%) for all pTs even below 20 GeV\n",
    "- D) The background efficiency peaks at 30 GeV with an efficiency of around 30%\n",
    "- E) The background efficiency peaks at high pT (> 50 GeV) with an efficiency of around 30%\n",
    "- F) The backround efficiency goes above 50% at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccad575-945f-4a3e-9e4c-deede7b3433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "bins = np.linspace(0, 128, 50)\n",
    "countsS,bins,_ = plt.hist(test[:][0][:,0][???],bins=bins,label=\"signal\", histtype='step', linewidth=2, color='crimson')\n",
    "countsB,bins,_ = plt.hist(test[:][0][:,0][???], bins=bins,label=\"background\", histtype='step', linewidth=2, color='gray')\n",
    "countsSC,bins,_ = plt.hist(test[:][0][:,0][???],bins=bins,label=\"signal + pass cut\", color='crimson', histtype='step', linewidth=2, linestyle='dashed')\n",
    "countsBC,bins,_ = plt.hist(test[:][0][:,0][???],bins=bins,label=\"background + pass cut\", color='gray', histtype='step', linewidth=2, linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.xlabel(r\"Leading Particle $p_{\\text{T}}$ [GeV]\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()\n",
    "\n",
    "bincenter = 0.5*(bins[1:] + bins[:-1])\n",
    "plt.plot(bincenter,countsSC/countsS,marker='.',label='signal')\n",
    "plt.plot(bincenter,countsBC/countsB,marker='.',label='bkg')\n",
    "plt.legend()\n",
    "plt.ylabel('Efficiency')\n",
    "plt.xlabel(r\"Leading Particle $p_{\\text{T}}$ [GeV]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a27e08-44f5-462f-b2c8-021761fa4976",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 10: Detecting a hidden signal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c5d12-5b95-4a05-8e6f-1f2c859dfae1",
   "metadata": {},
   "source": [
    "**Challenge:** Finally, let's see if we can use this NN to find a hidden signal in the data. We will construct a dataset with a mystery mass peak comprising two taus that is injected to background events at a level of 1.5% (i.e. 750 events in 50,000). Given the output variables of the two taus, find the invariant mass of the hidden signal. Report your answer as a number with precision `1e1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d095e1d-5888-42f8-bdb9-2dc46ddbee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### use pip to install pylorentz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365484d5-d191-4ff3-8cc9-61426d210e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylorentz import Momentum4\n",
    "\n",
    "def calculate_invariant_mass(vector_1, vector_2):\n",
    "    tau_1 = Momentum4.m_eta_phi_pt(vector_1[3], vector_1[1], vector_1[2], vector_1[0])\n",
    "    tau_2 = Momentum4.m_eta_phi_pt(vector_2[3], vector_2[1], vector_2[2], vector_2[0])\n",
    "    return (tau_1+tau_2).m\n",
    "\n",
    "def make_dataset(mass_function, dataset):\n",
    "    ### apply some basic quality cuts:\n",
    "    mask=(dataset[\"pt2\"].array() > 0)\n",
    "    mask=(abs(dataset[\"eta2\"].array()) < 2.1) & mask\n",
    "\n",
    "    ### define input variables\n",
    "    variables=[\"pt1\",\"eta1\",\"phi1\",\"m1\",\"pt2\",\"eta2\",\"phi2\",\"m2\"]\n",
    "    arr=0\n",
    "    idx=0\n",
    "    for var in variables:\n",
    "        pArr=dataset[var].array(library=\"np\")[mask]\n",
    "        if idx == 0: \n",
    "            arr = pArr\n",
    "            idx = idx + 1\n",
    "        else:\n",
    "            arr=np.vstack((arr,pArr))\n",
    "    arr = arr.T\n",
    "    \n",
    "    massc = lambda iarr: mass_function(iarr[0:4],iarr[4:8]) \n",
    "    masses = np.array([massc(p) for p in arr[0:50000]])\n",
    "    inputs1 = dataset['m1_inputs'].array()\n",
    "    inputs2 = dataset['m2_inputs'].array()\n",
    "    inputs1 = np.nan_to_num(np.asarray(inputs1)[mask][0:50000])\n",
    "    inputs2 = np.nan_to_num(np.asarray(inputs2)[mask][0:50000])\n",
    "    return masses,inputs1,inputs2\n",
    "\n",
    "def create_double_train_test_data(folder=\"./data/pset_1/\"):\n",
    "    import os\n",
    "    sig = uproot.open(os.path.join(folder, \"test_sig_v12_emseed.root\"))\n",
    "    bkg = uproot.open(os.path.join(folder, \"test_bkg_v12_emseed.root\"))\n",
    "    mvissig,in1_sig,in2_sig = make_dataset(calculate_invariant_mass,sig['ntuplePupDiTau']['tree'])\n",
    "    mvisbkg,in1_bkg,in2_bkg = make_dataset(calculate_invariant_mass,bkg['ntuplePupDiTau']['tree'])\n",
    "    masscut = ((mvissig > mvissig[0]/1.8) &  (mvissig < mvissig[0]/1.2))\n",
    "    mvissig          = mvissig[masscut]\n",
    "    in1_sig[masscut] = in1_sig[masscut]\n",
    "    in2_sig[masscut] = in2_sig[masscut]\n",
    "    mvis_mix = torch.tensor(np.append(mvisbkg[0:50000],mvissig[0:750]), dtype=torch.float32)\n",
    "    in1_mix  = torch.tensor(np.vstack([in1_bkg[0:50000],in1_sig[0:750]]), dtype=torch.float32)\n",
    "    in2_mix  = torch.tensor(np.vstack([in2_bkg[0:50000],in2_sig[0:750]]), dtype=torch.float32)\n",
    "    return mvis_mix, in1_mix, in2_mix\n",
    "                      \n",
    "### build the dataset\n",
    "m_mix, in1_mix, in2_mix = create_double_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d969806-3ac0-4cee-817c-ce2bb673a26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys_805",
   "language": "python",
   "name": "phys_805"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
